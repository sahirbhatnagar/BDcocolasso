---
title: "Introduction-to-BDcocolasso"
output:
  html_document:
    toc: true
    toc_float: false
    number_sections: true
    toc_depth: 4
    keep_md: false
editor_options: 
  chunk_output_type: console
params:
  NLessThanP: FALSE
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```




## Quick Start

We will use the simulated data which are provided in the package and can be loaded via:

```{r load-data}
# library(BDcocolasso)
devtools::load_all()
pacman::p_load(profvis)

data("simulated_data_missing")
data("simulated_data_additive")
data("simulated_data_missing_block")
data("simulated_data_additive_block")

```

These datasets correspond to corrupted datasets in the additive error setting and the missing data setting, and to partially corrupted datasets in additive error setting and missing data setting. In the missing data setting, those datasets contain NAs values, whereas in the additive error setting, they do not contain NAs values but the covariates are measured with additive error. 
We will note that it is essential to know the standard deviation corresponding to the additive error matrix in the additive error setting, in order to run the CoCoLasso algorithm.

## CoCoLasso algorithm

We can first perform a classic CoCoLasso regression. To do that, it is important to note that the datasets must be converted to a matrix:

```{r design-matrix}
if (params$NLessThanP) {
  n_used <- 100
} else {
  n_used <- 500
}
n_used
y_missing <- simulated_data_missing[1:n_used, 1]
length(y_missing)
Z_missing = simulated_data_missing[1:n_used, 2:dim(simulated_data_missing)[2]]
Z_missing = as.matrix(Z_missing)
dim(Z_missing)
n_missing <- dim(Z_missing)[1]
p_missing <- dim(Z_missing)[2]
y_missing = as.matrix(y_missing)
```

We can then fit the CoCoLasso model using our data. It is important to specify the type of noise (additive or missing) and the use of CoCoLasso (block=FALSE) or BD-CoCoLasso (block=TRUE). Here, `noise` is equal to `missing` because we have NAs values in the dataset. 
We can use two types of penalties : the classic lasso penalty (penalty="lasso") or the SCAD penalty (penalty="SCAD"). The latter should lead to less bias in the solution, when the signals are strong.
```{r fit-cocolasso}
set.seed(123456)
profvis::profvis({
  fit_missing.lasso = coco(Z=Z_missing,y=y_missing,n=n_missing,p=p_missing,
                           step=100,K=4,mu=10,tau=NULL, etol = 1e-4,
                           noise = "missing",block=FALSE, penalty="lasso")
})

set.seed(123456)
profvis::profvis({
  fit_missing.SCAD = coco(Z=Z_missing,y=y_missing,n=n_missing,p=p_missing,
                          step=100,K=4,mu=10,tau=NULL, etol = 1e-4,
                          noise = "missing",block=FALSE, penalty="SCAD")
})
```

It is possible to print the fitted object, so as to display the evolution of mean-squared error as a function of the lambda values:
```{r}
print(fit_missing.lasso)
```

It is also possible to display the coefficients obtained for all values of lambda, or for some specific values of lambda.
```{r}
coef(fit_missing.lasso, s=fit_missing.lasso$lambda.opt)
coef(fit_missing.SCAD, s=fit_missing.SCAD$lambda.opt)
```

It is also possible to obtain a prediction for new covariates. Let's simulate values following the same simulation pattern used to obtain Z_missing, and look at the obtained prediction. Default configuration is to use coefficients for lambda.sd:

```{r}
cov = cov_autoregressive(p_missing)
X = MASS::mvrnorm(1,mu=rep(0,p_missing),Sigma=cov)
beta = c(3,2,0,0,1.5,rep(0,p_missing - 5))
y = X %*% beta + rnorm(1,0,2)

y

```

Let's compare the prediction obtained with the lasso penalty and with the SCAD penalty :
```{r}
y_predict.sd.lasso <- predict(fit_missing.lasso, newx = X, type="response")
y_predict.sd.lasso
```

```{r}
y_predict.sd.SCAD <- predict(fit_missing.SCAD, newx = X, type="response")
y_predict.sd.SCAD
```

We can see that in this case using the SCAD penalty leads to less bias.

It is also possible to specify the lambda value with which we wish to perform the prediction:
```{r}
y_predict.opt <- predict(fit_missing.lasso, newx = X, type="response", lambda.pred = fit_missing.lasso$lambda.opt)
y_predict.opt
```

It is then possible to visualize the solution path of the coefficients:
```{r}
BDcocolasso::plotCoef(fit_missing.lasso)
BDcocolasso::plotCoef(fit_missing.SCAD)
```

It is also possible to visualize the mean squared error for all values of lambda:
```{r}
BDcocolasso::plotError(fit_missing.lasso)
BDcocolasso::plotError(fit_missing.SCAD)
```
The red stands for the mean squared error (without a constant term, which is why we obtain negative values), and the black depicts the standard deviation for each error value.
On each of the plots, the left dashed line represents the optimal lambda, while the right dashed line represents the lambda corresponding to the one-standard-error rule.


```{r, eval=FALSE}
knitr::knit_exit()
```


## Block-Descent CoCoLasso algorithm

We can fit the BDCoCoLasso model for both datasets using \code{block=TRUE}:
```{r design-bdcoco}

if (params$NLessThanP) {
  n_used <- 100
} else {
  n_used <- 500
}


n_used

p1 <- 180
p2 <- 20
y_missing <- simulated_data_missing_block[1:n_used,1]
Z_missing = simulated_data_missing_block[1:n_used,2:dim(simulated_data_missing_block)[2]]
Z_missing = as.matrix(Z_missing)
dim(Z_missing)
n_missing <- dim(Z_missing)[1]
p_missing <- dim(Z_missing)[2]
y_missing = as.matrix(y_missing)
set.seed(123456)

profvis::profvis({
  fit_missing = coco(Z=Z_missing,y=y_missing,n=n_missing,p=p_missing,p1=p1,
                     p2=p2,step=100,K=5,mu=10,tau=NULL,noise="missing",block=TRUE, 
                     penalty="lasso")
})

# this seed won't work. it hangs at lambda index: 53
# set.seed(12345)
# fit_missing = coco(Z=Z_missing,y=y_missing,n=n_missing,p=p_missing,p1=p1,p2=p2,step=100,K=5,mu=10,tau=NULL,noise="missing",block=TRUE, penalty="lasso")
```

Note that the algorithm requires that the first p1 columns of Z be the uncorrupted covariates, and that the last p2 columns be the corrupted covariates.
It it also important to keep in mind that this algorithm has a relatively high computational cost. In the example given above, it is normal that the code should run for a couple of minutes before obtaining a result. When the number of features reaches one thousand, the memory demand would be high.

We can plot the coefficients and the error for the missing data scenario. Here we should expect to have 6 non-zero coefficients, with pairs of coefficients having similar values, since data was simulated with beta = c(3,2,0,0,1.5,0,...,0,1.5,0,0,2,3).
```{r}
BDcocolasso::plotCoef(fit_missing)
BDcocolasso::plotError(fit_missing)
```

We can also use SCAD penalty for the Block-Descent-CoCoLasso :
```{r}
set.seed(123456)
profvis::profvis({
  fit_missing.SCAD = coco(Z=Z_missing,y=y_missing,n=n_missing,p=p_missing,p1=p1,
                          p2=p2,step=100,K=5,mu=10,tau=NULL,noise="missing",
                          block=TRUE, penalty="SCAD")
})
```

Once again, we get more erratic solution paths with slightly larger coefficients.

```{r}
BDcocolasso::plotCoef(fit_missing.SCAD)
BDcocolasso::plotError(fit_missing.SCAD)
```

We will note that here, due to the fact that there are only 3 coefficients that get activated, convergence should be very quick. We should select `lambda.opt` and not `lambda.sd` for getting good estimates for the coefficient values in this case. This would not be the case in a model where there are more activated coefficients with varying amplitude.

One important remark : using too high missing rate may lead to dysfunction of the algorithms. If the error "eigen(W, symmetric=TRUE) : infinite or NaN values" appears, this may indicate that you are using a matrix with too high missing rates. A good empirical threshold for missing values is 0.7.


## Session Info

```{r}
devtools::session_info()
```
